{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6dd6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shairozs\\.conda\\envs\\pytorch2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#coding=utf8\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, time, argparse\n",
    "\n",
    "from utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc660bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSICBottleneck:\n",
    "    def __init__(self, args):\n",
    "        if args.model == \"MLP\":\n",
    "            self.model  = MLP(args)\n",
    "        if args.model == \"signMLP\":\n",
    "            self.model  = signMLP(args)\n",
    "        if args.model == \"CNN\":\n",
    "            self.model  = CNN(args)\n",
    "        if args.model == \"VGG\":\n",
    "            self.model  = VGG(args)\n",
    "        if args.model == 'KAN':\n",
    "            self.model = MNISTChebyKAN2(degree=8)\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.batch_size = args.batchsize\n",
    "        self.lambda_0   = args.lambda_\n",
    "        self.sigma      = args.sigma_\n",
    "        self.extractor  = 'hsic'\n",
    "        self.last_linear = \"output_layer\"\n",
    "        self.HSIC = compute_HSIC(args.HSIC)\n",
    "        self.kernel = compute_kernel()\n",
    "        self.kernel_x = args.kernel_x\n",
    "        self.kernel_h = args.kernel_h\n",
    "        self.kernel_y = args.kernel_y\n",
    "        self.forward = args.forward\n",
    "        \n",
    "        self.opt = optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "#         self.opt = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "        self.iter_loss1, self.iter_loss2, self.iter_loss3 = [], [], []\n",
    "        self.track_loss1, self.track_loss2, self.track_loss3 = [], [], []\n",
    "        \n",
    "        self.loss = args.loss\n",
    "        if self.loss == \"mse\": self.output_criterion = nn.MSELoss()\n",
    "        elif self.loss == \"CE\": self.output_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def step(self, input_data, labels):\n",
    "        \n",
    "        labels_float = F.one_hot(labels, num_classes=10).float()\n",
    "        if self.forward == \"x\": Kx  = self.kernel(input_data, self.sigma, self.kernel_x)\n",
    "        Ky = self.kernel(labels_float, self.sigma, self.kernel_y)\n",
    "        \n",
    "        kernel_list = list()\n",
    "        y_pred, hidden_zs = self.model(input_data)\n",
    "        #print(y_pred.shape, [h.shape for h in hidden_zs])\n",
    "        \n",
    "        loss_LI = 0.\n",
    "        for num, feature in enumerate(hidden_zs): \n",
    "            kernel_list.append(self.kernel(feature, self.sigma, self.kernel_h))\n",
    "            ## Testing new features.\n",
    "            if args.Latinb == 1:\n",
    "                if num == (len(hidden_zs)-1) or feature.size(2) >= 4: continue\n",
    "                loss_LI += spatial_contrast(feature, args)*args.Latinb_lambda\n",
    "        \n",
    "        total_loss1, total_loss2, total_loss3 = 0., 0., 0.\n",
    "        for num, feature in enumerate(kernel_list):\n",
    "            if num == (len(hidden_zs)-1): \n",
    "                if self.forward == \"h\": total_loss1 += self.HSIC(feature, kernel_list[num-1], self.batch_size, device)\n",
    "                elif self.forward == \"x\": total_loss1 += self.HSIC(feature, Kx, self.batch_size, device)\n",
    "                if self.loss == \"mse\": total_loss3 += self.output_criterion(hidden_zs[-1], labels_float)\n",
    "                elif self.loss == \"CE\": total_loss3 += self.output_criterion(hidden_zs[-1], labels)\n",
    "            elif num == 0:\n",
    "                if self.forward == \"x\": total_loss1 += self.HSIC(feature, Kx, self.batch_size, device)\n",
    "                total_loss2 += - self.lambda_0*self.HSIC(feature, Ky, self.batch_size, device)\n",
    "            else:\n",
    "                if self.forward == \"h\": total_loss1 += self.HSIC(feature, kernel_list[num-1], self.batch_size, device)\n",
    "                elif self.forward == \"x\": total_loss1 += self.HSIC(feature, Kx, self.batch_size, device)\n",
    "                total_loss2 += - self.lambda_0*self.HSIC(feature, Ky, self.batch_size, device)\n",
    "        \n",
    "        if self.forward == \"h\" or self.forward == \"x\": \n",
    "            total_loss = total_loss1 + total_loss2 + total_loss3 + loss_LI\n",
    "            self.iter_loss1.append(total_loss1.item())\n",
    "        if self.forward == \"n\": \n",
    "            total_loss = total_loss2 + total_loss3 + loss_LI\n",
    "            self.iter_loss1.append(-1)\n",
    "        self.opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.opt.step()\n",
    "                \n",
    "        self.iter_loss2.append(total_loss2.item())\n",
    "        self.iter_loss3.append(total_loss3.item())\n",
    "        \n",
    "    def update_loss(self):\n",
    "        self.track_loss1.append(np.mean(self.iter_loss1))\n",
    "        self.track_loss2.append(np.mean(self.iter_loss2))\n",
    "        self.track_loss3.append(np.mean(self.iter_loss3))\n",
    "        self.iter_loss1, self.iter_loss2, self.iter_loss3 = [], [], []\n",
    "    \n",
    "    def tune_output(self, input_data, labels):\n",
    "        self.model.train()\n",
    "        if self.loss == \"mse\":\n",
    "            one_hot_labels = F.one_hot(labels, num_classes=10)\n",
    "            labels = F.one_hot(labels, num_classes=10).float()\n",
    "        \n",
    "        y_pred, hidden_zs = self.model(input_data)\n",
    "        total_loss = self.output_criterion(hidden_zs[-1], labels)\n",
    "        self.opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a278138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan2_results_degree_eight.csv\n",
      "Model trainable parameters:  2249994\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 0. \t Training  ACC: 0.8089. \t Testing ACC: 0.8084\n",
      "27.61\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 2. \t Training  ACC: 0.8485. \t Testing ACC: 0.8516\n",
      "41.08\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 4. \t Training  ACC: 0.8623. \t Testing ACC: 0.8664\n",
      "41.91\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 6. \t Training  ACC: 0.8687. \t Testing ACC: 0.8702\n",
      "40.60\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 8. \t Training  ACC: 0.8794. \t Testing ACC: 0.8802\n",
      "40.17\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 10. \t Training  ACC: 0.8890. \t Testing ACC: 0.8920\n",
      "40.77\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 12. \t Training  ACC: 0.8982. \t Testing ACC: 0.8989\n",
      "41.39\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 14. \t Training  ACC: 0.8875. \t Testing ACC: 0.8872\n",
      "40.57\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 16. \t Training  ACC: 0.8971. \t Testing ACC: 0.8990\n",
      "41.45\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 18. \t Training  ACC: 0.8939. \t Testing ACC: 0.8925\n",
      "39.47\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 20. \t Training  ACC: 0.8913. \t Testing ACC: 0.8908\n",
      "38.58\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 22. \t Training  ACC: 0.8954. \t Testing ACC: 0.8937\n",
      "40.18\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 24. \t Training  ACC: 0.8926. \t Testing ACC: 0.8858\n",
      "37.84\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 26. \t Training  ACC: 0.8690. \t Testing ACC: 0.8671\n",
      "38.52\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 28. \t Training  ACC: 0.8725. \t Testing ACC: 0.8690\n",
      "37.58\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 30. \t Training  ACC: 0.8910. \t Testing ACC: 0.8869\n",
      "38.18\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 32. \t Training  ACC: 0.8716. \t Testing ACC: 0.8675\n",
      "37.68\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 34. \t Training  ACC: 0.8804. \t Testing ACC: 0.8758\n",
      "37.50\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 36. \t Training  ACC: 0.8897. \t Testing ACC: 0.8802\n",
      "37.82\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 38. \t Training  ACC: 0.8704. \t Testing ACC: 0.8609\n",
      "37.60\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 40. \t Training  ACC: 0.8716. \t Testing ACC: 0.8599\n",
      "37.83\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 42. \t Training  ACC: 0.8796. \t Testing ACC: 0.8695\n",
      "38.53\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 44. \t Training  ACC: 0.8775. \t Testing ACC: 0.8704\n",
      "37.22\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 46. \t Training  ACC: 0.8809. \t Testing ACC: 0.8723\n",
      "37.54\n",
      "Input shape:  torch.Size([128, 784])\n",
      "Target shape:  torch.Size([128])\n",
      "EPOCH 48. \t Training  ACC: 0.8707. \t Testing ACC: 0.8650\n",
      "37.63\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default=\"mnist\")\n",
    "    parser.add_argument('--model', type=str, default=\"KAN\")\n",
    "    parser.add_argument('--loss', type=str, default=\"CE\")\n",
    "    parser.add_argument('--BP', type=int, default=0)\n",
    "    parser.add_argument('--HSIC', type=str, default=\"nHSIC\")\n",
    "    parser.add_argument('--kernel_x', type=str, default=\"rbf\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--kernel_h', type=str, default=\"rbf\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--kernel_y', type=str, default=\"student\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--sigma_', type=int, default=10)\n",
    "    parser.add_argument('--lambda_', type=int, default=100)\n",
    "    parser.add_argument('--batchsize', type=int, default=128)\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "    parser.add_argument('--bn_affine', type=int, default=1)\n",
    "    parser.add_argument('--forward', type=str, default=\"n\", choices=[\"x\", \"h\", \"n\"])\n",
    "    \n",
    "    # Testing.\n",
    "    parser.add_argument('--Latinb', type=int, default=0, choices=[0, 1])\n",
    "    parser.add_argument('--Latinb_lambda', type=float, default=1.)\n",
    "    parser.add_argument('--Latinb_type', type=str, default=\"f\", choices=[\"f\", \"n\"])\n",
    "        \n",
    "    args, _ = parser.parse_known_args()\n",
    "    filename = 'kan2_results_degree_eight.csv'#get_filename(args)\n",
    "    print(filename)\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    device = \"cuda:{}\".format(args.device)\n",
    "    batch_size = args.batchsize\n",
    "    train_loader, test_loader = load_data(args)\n",
    "    \n",
    "    logs = list()\n",
    "    hsic = HSICBottleneck(args)\n",
    "    start = time.time()\n",
    "    get_loss = list()\n",
    "    print(\"Model trainable parameters: \", sum(p.numel() for p in hsic.model.parameters() if p.requires_grad))\n",
    "\n",
    "    for epoch in range(50):\n",
    "        hsic.model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.view(args.batchsize, -1)\n",
    "            hsic.step(data.view(args.batchsize, -1).to(device), target.to(device))\n",
    "            hsic.tune_output(data.view(args.batchsize, -1).to(device), target.to(device))\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Input shape: \", data.shape)\n",
    "            print(\"Target shape: \", target.shape)\n",
    "            show_result(hsic, train_loader, test_loader, epoch, logs, device)\n",
    "            print(\"{:.2f}\".format(time.time()-start))\n",
    "            start = time.time()\n",
    "\n",
    "    txt_path = os.path.join(\".\\\\\", filename+\".csv\")\n",
    "    df = pd.DataFrame(logs)\n",
    "    #df.to_csv(txt_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f1219a-f508-4731-8e31-594ebe62fd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\kan_results.csv.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d7d02f-b45b-45f6-b630-ca1d7e44e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_results\n",
      "EPOCH 0. \t Training  ACC: 0.9042. \t Testing ACC: 0.9103\n",
      "25.75\n",
      "EPOCH 2. \t Training  ACC: 0.9334. \t Testing ACC: 0.9342\n",
      "38.61\n",
      "EPOCH 4. \t Training  ACC: 0.9401. \t Testing ACC: 0.9400\n",
      "38.58\n",
      "EPOCH 6. \t Training  ACC: 0.9440. \t Testing ACC: 0.9486\n",
      "38.14\n",
      "EPOCH 8. \t Training  ACC: 0.9462. \t Testing ACC: 0.9470\n",
      "37.17\n",
      "EPOCH 10. \t Training  ACC: 0.9496. \t Testing ACC: 0.9490\n",
      "37.64\n",
      "EPOCH 12. \t Training  ACC: 0.9502. \t Testing ACC: 0.9494\n",
      "38.00\n",
      "EPOCH 14. \t Training  ACC: 0.9521. \t Testing ACC: 0.9523\n",
      "37.77\n",
      "EPOCH 16. \t Training  ACC: 0.9523. \t Testing ACC: 0.9500\n",
      "37.45\n",
      "EPOCH 18. \t Training  ACC: 0.9535. \t Testing ACC: 0.9526\n",
      "37.83\n",
      "EPOCH 20. \t Training  ACC: 0.9531. \t Testing ACC: 0.9516\n",
      "37.59\n",
      "EPOCH 22. \t Training  ACC: 0.9538. \t Testing ACC: 0.9497\n",
      "37.41\n",
      "EPOCH 24. \t Training  ACC: 0.9566. \t Testing ACC: 0.9544\n",
      "37.59\n",
      "EPOCH 26. \t Training  ACC: 0.9567. \t Testing ACC: 0.9541\n",
      "38.13\n",
      "EPOCH 28. \t Training  ACC: 0.9569. \t Testing ACC: 0.9552\n",
      "37.52\n",
      "EPOCH 30. \t Training  ACC: 0.9582. \t Testing ACC: 0.9572\n",
      "37.86\n",
      "EPOCH 32. \t Training  ACC: 0.9570. \t Testing ACC: 0.9526\n",
      "37.46\n",
      "EPOCH 34. \t Training  ACC: 0.9558. \t Testing ACC: 0.9530\n",
      "37.95\n",
      "EPOCH 36. \t Training  ACC: 0.9570. \t Testing ACC: 0.9548\n",
      "37.80\n",
      "EPOCH 38. \t Training  ACC: 0.9580. \t Testing ACC: 0.9533\n",
      "37.56\n",
      "EPOCH 40. \t Training  ACC: 0.9576. \t Testing ACC: 0.9559\n",
      "37.20\n",
      "EPOCH 42. \t Training  ACC: 0.9591. \t Testing ACC: 0.9563\n",
      "38.07\n",
      "EPOCH 44. \t Training  ACC: 0.9587. \t Testing ACC: 0.9565\n",
      "37.94\n",
      "EPOCH 46. \t Training  ACC: 0.9590. \t Testing ACC: 0.9562\n",
      "38.40\n",
      "EPOCH 48. \t Training  ACC: 0.9588. \t Testing ACC: 0.9587\n",
      "37.48\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default=\"mnist\")\n",
    "    parser.add_argument('--model', type=str, default=\"MLP\")\n",
    "    parser.add_argument('--loss', type=str, default=\"CE\")\n",
    "    parser.add_argument('--BP', type=int, default=0)\n",
    "    parser.add_argument('--HSIC', type=str, default=\"nHSIC\")\n",
    "    parser.add_argument('--kernel_x', type=str, default=\"rbf\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--kernel_h', type=str, default=\"rbf\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--kernel_y', type=str, default=\"student\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--sigma_', type=int, default=10)\n",
    "    parser.add_argument('--lambda_', type=int, default=100)\n",
    "    parser.add_argument('--batchsize', type=int, default=128)\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "    parser.add_argument('--bn_affine', type=int, default=1)\n",
    "    parser.add_argument('--forward', type=str, default=\"n\", choices=[\"x\", \"h\", \"n\"])\n",
    "    \n",
    "    # Testing.\n",
    "    parser.add_argument('--Latinb', type=int, default=0, choices=[0, 1])\n",
    "    parser.add_argument('--Latinb_lambda', type=float, default=1.)\n",
    "    parser.add_argument('--Latinb_type', type=str, default=\"f\", choices=[\"f\", \"n\"])\n",
    "        \n",
    "    args, _ = parser.parse_known_args()\n",
    "    filename = 'mlp_results'#get_filename(args)\n",
    "    print(filename)\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    device = \"cuda:{}\".format(args.device)\n",
    "    batch_size = args.batchsize\n",
    "    train_loader, test_loader = load_data(args)\n",
    "    \n",
    "    logs = list()\n",
    "    hsic = HSICBottleneck(args)\n",
    "    start = time.time()\n",
    "    get_loss = list()\n",
    "    print(\"Model trainable parameters: \", sum(p.numel() for p in hsic.model.parameters() if p.requires_grad))\n",
    "    for epoch in range(50):\n",
    "        hsic.model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.view(args.batchsize, -1)\n",
    "            hsic.step(data.view(args.batchsize, -1).to(device), target.to(device))\n",
    "            hsic.tune_output(data.view(args.batchsize, -1).to(device), target.to(device))\n",
    "        if epoch % 2 == 0:\n",
    "            show_result(hsic, train_loader, test_loader, epoch, logs, device)\n",
    "            print(\"{:.2f}\".format(time.time()-start))\n",
    "            start = time.time()\n",
    "\n",
    "    txt_path = os.path.join(\".\\\\\", filename+\".csv\")\n",
    "    df = pd.DataFrame(logs)\n",
    "    \n",
    "    #df.to_csv(txt_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf1b4e1-2903-449a-85a3-abf63d36e74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252682"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in hsic.model.parameters() if p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
